
\section{Our Method}
%
\subsection{GBCNet: Model Architecture}
%
%Unlike CT or MRI, \usg images suffer from much higher noise levels and artifacts such as shadow due to \gb stones. 
The artifacts in \usg images often result in multiple spurious areas in a \usg image with very similar visual traits as the \gb region, leading to a disappointing performance by the baseline classifiers. 
%We have developed a CNN-based model, referred to as GBCNet, which can successfully detect \gbc, despite all the challenges described above. 
GBCNet selects candidate regions of interest (\rois) from the \usg to mitigate the influence of spurious artifacts like shadows and then uses a multi-scale, second-order pooling-based (MS-SoP) classifier on the \rois. \cref{fig-arch-overview} presents a conceptual diagram of the architecture. 
%We train a deep object detector to localize the \gb in the \usg images and generate the candidate ROIs to mitigate the influence of spurious artifacts. 
%The ROIs are cropped from the RGB images and passed to the MS-SoP classifier, to predict \gb malignancy.
At test time, the detector may predict multiple \rois overlapping with the \gb. Occasionally, the detection network may fail to localize the \rois. In this case, we pass the entire image to the classifier. The proposed MS-SoP classifier exploits a range of spatial scales 
%through multiple receptive fields during the feature encoding. In addition, the classifier utilizes 
and second-order statistics to generate rich features from \usg images to learn the characteristics of malignancy. We run the classifier for each candidate region during inference and aggregate the predicted labels to compute the prediction for the entire image. If any of the \rois is classified as malignant, the image as a whole is classified as malignant. If all the regions are predicted to be normal, the image is classified as normal. In all other cases, the image is predicted to be benign. 
%\par We observe that the above configuration of \gbCNet, though improving the detection of malignancy, still has a high false-positive rate. Our analysis shows that this is due to the texture bias of neural network models, which has been reported for the natural images as well \cite{geirhos2018Texture}. Inspired by human visual acuity, we propose a novel customized curriculum training regimen for training the classifier module of GBCNet. The proposed training process mitigates the texture bias and helps GBCNet reach the reported performance level. In the following subsections, we discuss different components of the GBCNet.

\mypara{Candidate \roi Selection}
%
We used deep object detection networks to localize the \gb region in a \usg image. The predicted bounding boxes serve as candidate regions of interest and mitigate the adverse effect of noise and artifacts from non-\gb regions during the classification. For training the \roi selection models, we use only two classes - background and the \gb region. In this stage, we only detect the \gb and do not classify them as malignant or non-malignant. In principle, it is possible to do both in a single stage, but we observed 
%that the candidate regions suggested by first stage region proposal network are often shifted from the \gb region, and classifying malignancy based upon them leads to poorer results. After the second stage the regions gets refined to a greater accuracy, and we observed 
that using a separate classifier on the focused \rois leads to better accuracy. We note that our findings regarding the superiority of using classification on focused regions, instead of over the entire image, are consistent with other recent works \cite{lancet_pancreas, cao2019BreastLesion, fan2020inf, sirinukunwattana2016locality, eccv2020_devil_in_classification}. Prior studies demonstrate that modern object detection architectures such as YOLO \cite{yolov4} or Faster-RCNN \cite{fasterrcnn} can detect breast lesions in \usg images  \cite{cao2019BreastLesion}. On the other hand, recently proposed anchor-free approaches, such as Reppoints \cite{reppoints}, and CentripetalNet \cite{centripetalnet} can detect unconventionally sized objects such as \gb. Hence, we experimented with all the above approaches for \roi selection in our framework. 

\mypara{MS-SoP Classifier}
%
Feature extraction in deep neural networks typically rely on first-order statistics, which performs unsatisfactorily in modalities like \usg. The presence of noise and artifacts along with ambiguous organ boundaries add to the complexities of detecting \gbc from \usg images. 
Modeling higher-order statistics has gained popularity in recent years due to its enhanced ability to capture complex features and non-linearity in deep neural networks \cite{gao2019global, li2017second, zoumpourlis2017non}. %The presence of noise and artifacts along with ambiguous organ boundaries add to the complexities of detecting \gbc from ultrasound images. 
Ning \etal \cite{ning2020multi} recently used higher-order feature fusion for classifying breast lesions. They have used RGB image patches of three fixed scales at the input layer. We take the idea further and develop a novel multi-scale, second-order pooling (MS-SoP) layer to encode rich features suitable for malignant \gb detection. In contrast to \cite{ning2020multi}, we exploit feature maps of multiple scales in all the intermediate layers to learn a rich representation. The proposed MS-SoP layers can be conveniently plugged into any \cnn backbone.
%Further, instead of using the features at different resolutions, we use the multiple receptive fields to capture features at different scales. 
The MS-SoP classifier contains $16$ MS-SoP layers as the backbone, followed by global average pooling and a fully connected classification head. 
Each block uses multiple scales and second-order pooling to encode robust feature representation. %The 1$\times$1 convolutions resize the feature depth for computational efficiency. 
The MS-SoP classifier takes input of size $224\times224\times3$. As input \usg images are grayscale, we copy the resized image to all three channels. Using a first layer that directly takes as input the grayscale was possible in principle but would have denied us the opportunity to use pre-trained backbones. Consistent with the ones reported in the literature \cite{alzubaidi2020transferlearning, cheng2017transfer}, we observe that cross-domain fine-tuning of a pre-trained classifier gives better accuracy than training from scratch. 
We use the Categorical Cross-Entropy loss to train the classifier. 

%We have used a $7\times 7$ convolution layer and $16$ multi-scale, second-order pooling-based convolutional layers as feature extraction backbone of the classification network. The feature extraction backbone is followed by global average pooling, and a fully connected layer completes the classifier network. \cref{fig:gbc_block} presents a pictorial overview of the classifier, highlighting the feature extraction block. Each block uses multiple scales and second-order pooling to encode robust feature representation. The 1$\times$1 convolutions resize the feature map depth for computational efficiency. %Our designed classifier network contains 26.9M parameters. 
%The final softmax layer outputs probability for three classes representing normal, benign, and malignant \gb. The classifier takes input of size $224 \times 224 \times 3$. We crop the candidate ROIs from the input image as predicted by the region selection network and resize them to feed the classifier. As input \usg images are grayscale, we copy the resized image to all three channels. Using a first layer that directly takes as input the grayscale was possible in principle but would have denied us the opportunity to use pre-trained backbones. Consistent with the ones reported in the literature \cite{alzubaidi2020transferlearning, cheng2017transfer}, we observe that cross-domain fine-tuning of a pre-trained classifier gives better accuracy than training from scratch. We used the standard Categorical Cross-Entropy loss to train the classifier unit. 

