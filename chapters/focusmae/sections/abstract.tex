%\begin{abstract}
In recent years, automated Gallbladder Cancer (GBC) detection has gained the attention of researchers. Current state-of-the-art (SOTA) methodologies relying on ultrasound sonography (USG) images exhibit limited generalization, emphasizing the need for transformative approaches. We observe that individual USG frames may lack sufficient information to capture disease manifestation. This study advocates for a paradigm shift towards video-based GBC detection, leveraging the inherent advantages of spatiotemporal representations. Employing the Masked Autoencoder (MAE) for representation learning, we address shortcomings in conventional image-based methods. 
%While video-based GBC classification demonstrates improvements, a meticulous examination reveals an opportunity for refinement to the random masking intrinsic to MAE. 
We propose a novel design called \focusmae to systematically bias the selection of masking tokens from high-information regions, fostering a more refined representation of malignancy. Additionally, we contribute the most extensive USG video dataset for GBC detection. We also note that, this is the first study on USG video-based GBC detection. We validate the proposed methods on the curated dataset, and report a new state-of-the-art (SOTA) accuracy of 96.4\% for the GBC detection problem, against an accuracy of 84\% by current SOTA - GBCNet, and RadFormer. We further demonstrate the generality of the proposed \focusmae on a public CT-based Covid detection dataset, reporting an improvement in accuracy by 2.2\% over current baselines. The source code and pretrained models are available \footnote{\href{https://anonymous.4open.science/r/FocusMAE/}{https://anonymous.4open.science/r/FocusMAE}}.
    %Video-based disease detection presents a superior approach over image-based methods in modalities such as Ultrasound (USG), where individual frames may lack sufficient information to capture disease manifestation. Masked Autoencoders (MAEs) have shown promise in learning video representations by reconstructing masked input data from visible data. However, current MAE approaches primarily utilize random patch, frame, or tube-based masking strategies for token selection, and lacking in explainability. While some existing techniques propose adaptive masking strategies for natural videos based on motion guidance or expected reconstruction error, they show suboptimal performance for medical videos. This paper introduces a novel approach by leveraging a mutual information-based approach and a B-cos vision transformer to align tokens with high-information spatiotemporal regions, and effectively enhance the explainability within the VideoMAE framework. We validate the proposed approach in the context of Gallbladder Cancer (GBC) detection in USG videos. We further extend the application to Polyp detection in colonoscopy videos, demonstrating the generality and efficacy of the proposed method.
%\end{abstract}